# 信用违约风险预测	

**比赛日期**：2020.4.29 ~ 2020.5.12

**B榜成绩**：0.780，排名X（官方还未给出）

**赛题任务**： 主办方提供两个数据集（训练数据集和评分数据集），包含用户标签数据、过去60天的交易行为数据、过去30天的APP行为数据。选手使用训练集构建信用违约预测模型，并将模型应用在评分数据集上，预测评分集中每个用户的违约概率。评分指标为AUC。

## 方案总览

流程图：

![模型构建流程](.\img\模型构建流程.png)



## 框架细节

1. **探索性数据分析EDA**

此部分主要通过可视化工具和统计方法对数据集进行直观分析，对数据集有整体认识，为后续操作做好基础，主要分析内容：

- 样本数量；
- Label分布；
- 数据字段类型；
- 数据字段分布；
- 数据字段的异常值、缺失值；
- 训练集和测试集的分布差异；
- 字段和label的分布关系；
- 字段相关性分析；
- 有时间特征的可以按不同时间窗口划分观察；
- ……

探索性分析的一些结论，可能对后续特征工程有指导作用：

+ beh 字段缺失过多，可能没必要使用。
+ tag表中部分字段含有“\N, ~”等符号，意义不明，需要根据列类型进行针对性的处理。
+ trd表中唯一和金钱有关的字段是 cny_trx_amt， 应重点挖掘此项特征。
+ 有些用户在00：00~00：30之间交易频繁，可能是参与秒杀？



对比0.784的方案（参考链接在文章末尾），本方案还欠缺了一些观察：

1. 测试集较小，在A榜阶段，模型不应该过多注意线上成绩，否则太容易过拟合B榜成绩可能不好，线下验证非常重要。（**我是在切换B榜后才开始使用验证集，前期方案可能过拟合，做了一些无用功**）
2. 交易行为数据trd和APP行为数据beh两个表中用户id较少，不能基本覆盖tag中的所有用户，做出的特征缺失值会较多，所以这两个表不一定是必需使用的数据。（**并没有去观察trd， beh的id对应情况，直接使用pd.merge， 今后需要注意**）



2. **数据预处理**

预处理部分主要是两个操作：填充缺失值和字段变量转换。

1）缺失值填充：对于tag表中的缺失值， 如果是类别型的，直接用one-hot编码，“\N”单独作为一列，可以不做处理；如果是数值型的，填充np.nan。（LGB 和 XGB会自动处理缺失值）

2）字段变量转换

- 对于类别类字段，如学历、学位、性别、标识等，使用labelencoder编码或者one-hot编码；
- 对于等级代码和连续型字段，如持卡天数、张书、承受风险级别等，将“\N”转化为0或-1。



3. **特征工程与特征筛选**

本次比赛其实我没有太多的时间参与，重点挖掘的特征在trd的金额表上，如下图所示：

![特征工程](.\img\特征工程.png)

**而0.784的方案中主要提取了RFM模型的特征：**

RFM模型是衡量客户价值和客户创利能力的重要工具和手段，客户主要包含三个重要属性：最近一次消费 (Recency)， 消费频率 (Frequency)， 消费金额 (Monetary)。

参考链接： <https://wiki.mbalib.com/zh-tw/RFM%E6%A8%A1%E5%9E%8B>



trd表中提取的最近一次交易R、交易频率F和交易金额M类特征是非常有效的，主要包括：

- 粗粒度刻画：每个id最后一次交易时间、最后一次交易时间是否晚于平均值、交易总次数、多少天有交易行为、交易的总金额、平均每天交易次数、平均每天交易金额、平均每次交易金额；
- 进一步细粒度刻画：按照不同交易方向，不同交易方式，以及不同的一级二级交易分类分别聚合分组，计算每个id的最后一次交易时间、最后一次交易时间是否晚于平均值、交易总次数、平均每天交易次数、总金额数、平均每天金额、平均每次金额；
- 再进一步细粒度：trd中包含5月和6月两个月用户交易行为，单独考虑信贷用户每月的行为还是很重要的，所以分别对每个月做1)     2)中所述特征统计。

**而我的方案只包含上述RFM模型的M类特征。**

4. **模型训练与优化**

a榜的时候我并没有考虑到评分数据集较小，容易造成过拟合的特点，没有采用验证集。b榜的时候改进了这项流程，训练集：验证集 = 8 ：2。在训练集阶段我使用的是XGboost，但是在b榜的时候发现同样情况下XGB的千分位要比LightGBM要低，因此在B榜中就采用了LightGBM。最后阶段将经过调参后的XGB 和 LightGBM的结果进行线性混合，最高成绩为0.780。

上分迭代过程如下图：

![上分迭代时间轴](.\img\上分迭代时间轴.png)



## 总结

1. B榜提交限制次数，总共才9次, 必须使用线下验证结果，而不是针对A榜过拟合调参。
2. A榜的数据分布可能有些问题，在8 9号上验证集的时候发现线上线下的变化趋势不太一致，而B榜则变化相对一致。
3. 线下验证集的构造非常重要，由于数据量不大， 使用K折交叉验证方式平滑预测结果。
4. 细致的数据预处理对本次比赛也有非常重要的作用，因为时间所限，我的预处理是非常粗糙的，后面比赛进入万分位争夺的时候就是后悔。
5. 同样地，没有经过特征筛选，可能会造成过拟合。（可以是使用树模型，选取top100 importance的特征）
6. 模型的版本控制需要提升，整个比赛中间结果保存的有些凌乱。

### Reference：

0.784方案： <https://zhuanlan.zhihu.com/p/140017918>